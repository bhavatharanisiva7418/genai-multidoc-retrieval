{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f9a63e7",
   "metadata": {
    "height": 931
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. By increasing the context window size, the perplexity decreases, indicating the effectiveness of the fine-tuning method. Additionally, the models are extended to handle extremely large context lengths, with promising results, although there is some perplexity degradation on small context sizes for the extended models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to a decrease in perplexity, indicating the effectiveness of the fine-tuning method. The models are also extended to handle extremely large context lengths, showing promising results. However, there is some perplexity degradation on small context sizes for the extended models.\n",
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for fine-tuning models to longer context lengths without altering their original architectures. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. Additionally, improvements have been made to LongLoRA through enhancements like LoRA+ and Position Interpolation, addressing challenges in adapting LLMs from short to long context lengths and demonstrating promising results in tasks like topic retrieval and passkey accuracy.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for fine-tuning models to longer context lengths without altering their original architectures. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. Additionally, improvements have been made to LongLoRA through enhancements like LoRA+ and Position Interpolation, addressing challenges in adapting LLMs from short to long context lengths and demonstrating promising results in tasks like topic retrieval and passkey accuracy.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It allows a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. This approach significantly outperforms state-of-the-art language models and retrieval-augmented models on various tasks, showing improvements in factuality, citation accuracy, and overall generation quality.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context lengths of Large Language Models (LLMs) while minimizing computational costs and training time. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for fine-tuning models to longer context lengths without altering their original architectures. LongLoRA has shown strong empirical results across various tasks and is compatible with existing techniques like Flash-Attention2. Additionally, improvements have been made to LongLoRA through enhancements like LoRA+ and Position Interpolation, addressing challenges in adapting LLMs from short to long context lengths and demonstrating promising results in tasks like topic retrieval and passkey accuracy.\n",
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_metagpt with args: {\"query\": \"comparison of evaluation dataset between MetaGPT and SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "MetaGPT is evaluated on HumanEval and MBPP datasets, which consist of handwritten programming tasks covering various aspects of software development. The evaluation metrics for HumanEval and MBPP focus on the functional accuracy of generated code. On the other hand, SWE-Bench prioritizes practical use and evaluates performance through human evaluations, cost analysis, code statistics, productivity metrics, and human revision cost.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. \n",
      "\n",
      "MetaGPT is evaluated on HumanEval and MBPP datasets, which consist of handwritten programming tasks covering various aspects of software development. The evaluation metrics for HumanEval and MBPP focus on the functional accuracy of generated code. \n",
      "\n",
      "In comparison, SWE-Bench prioritizes practical use and evaluates performance through human evaluations, cost analysis, code statistics, productivity metrics, and human revision cost.\n",
      "assistant: The evaluation dataset used in MetaGPT is the SoftwareDev dataset, which consists of 70 diverse software development tasks. \n",
      "\n",
      "MetaGPT is evaluated on HumanEval and MBPP datasets, which consist of handwritten programming tasks covering various aspects of software development. The evaluation metrics for HumanEval and MBPP focus on the functional accuracy of generated code. \n",
      "\n",
      "In comparison, SWE-Bench prioritizes practical use and evaluates performance through human evaluations, cost analysis, code statistics, productivity metrics, and human revision cost.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")\n",
    "\n",
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ce647",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "## Name:BHAVATHARANI S\n",
    "## Reg no:212223230032"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
